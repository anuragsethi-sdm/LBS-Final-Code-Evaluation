{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "966e32eb-3bb5-413e-8365-8f3855d294ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "def load_data(annotations_path='C:/python311/csv files/annotations', images_path=r'C:/python311/csv files/images'):\n",
    "     return images, labels, bounding_boxes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "537845f8-8021-43c2-a859-be00bc50e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f61679-e945-436a-a823-0ef055065147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 238ms/step - accuracy: 0.7628 - loss: 0.9251 - val_accuracy: 0.8957 - val_loss: 0.2646\n",
      "Epoch 2/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 215ms/step - accuracy: 0.9372 - loss: 0.1709 - val_accuracy: 0.9384 - val_loss: 0.1602\n",
      "Epoch 3/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 213ms/step - accuracy: 0.9408 - loss: 0.1438 - val_accuracy: 0.9431 - val_loss: 0.1422\n",
      "Epoch 4/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - accuracy: 0.9663 - loss: 0.1070 - val_accuracy: 0.8957 - val_loss: 0.2864\n",
      "Epoch 5/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - accuracy: 0.9604 - loss: 0.1472 - val_accuracy: 0.9479 - val_loss: 0.1546\n",
      "Epoch 6/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - accuracy: 0.9606 - loss: 0.0937 - val_accuracy: 0.9100 - val_loss: 0.2035\n",
      "Epoch 7/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 225ms/step - accuracy: 0.9748 - loss: 0.1020 - val_accuracy: 0.9384 - val_loss: 0.1257\n",
      "Epoch 8/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 211ms/step - accuracy: 0.9775 - loss: 0.0706 - val_accuracy: 0.9431 - val_loss: 0.1236\n",
      "Epoch 9/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 206ms/step - accuracy: 0.9823 - loss: 0.0650 - val_accuracy: 0.9431 - val_loss: 0.1287\n",
      "Epoch 10/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - accuracy: 0.9765 - loss: 0.0606 - val_accuracy: 0.9242 - val_loss: 0.2191\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_dir = r\"C:\\python311\\csv files\\images\"\n",
    "annotation_dir = r\"C:\\python311\\csv files\\annotations\"\n",
    "output_dir = \"processed_dataset/\"\n",
    "\n",
    "resized_img = cv2.resize(img, (50, 50))  # if using OpenCV\n",
    "\n",
    "def image_generator(image_paths, batch_size):\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for path in image_paths:\n",
    "            img = load_image(path)\n",
    "            batch_images.append(img)\n",
    "            batch_labels.append(get_label(path))\n",
    "            if len(batch_images) == batch_size:\n",
    "                yield np.array(batch_images) / 255.0, np.array(batch_labels)\n",
    "                batch_images.clear()\n",
    "                batch_labels.clear()\n",
    "\n",
    "\n",
    "data = da.from_array(np.array(data), chunks=(100, 100, 100, 3))\n",
    "\n",
    "X = np.memmap('filename.dat', dtype='float32', mode='w+', shape=(num_images, height, width, channels))\n",
    "\n",
    "import gc\n",
    "del unnecessary_variable\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for label in ['with_mask', 'without_mask']:\n",
    "    os.makedirs(os.path.join(output_dir, label), exist_ok=True)\n",
    "\n",
    "for xml_file in os.listdir(annotation_dir):\n",
    "    if not xml_file.endswith('.xml'):\n",
    "        continue\n",
    "\n",
    "    tree = ET.parse(os.path.join(annotation_dir, xml_file))\n",
    "    root = tree.getroot()\n",
    "    filename = root.find('filename').text\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        x1 = int(bbox.find('xmin').text)\n",
    "        y1 = int(bbox.find('ymin').text)\n",
    "        x2 = int(bbox.find('xmax').text)\n",
    "        y2 = int(bbox.find('ymax').text)\n",
    "        face_crop = image[y1:y2, x1:x2]\n",
    "        save_path = os.path.join(output_dir, label, filename)\n",
    "        cv2.imwrite(save_path, face_crop)\n",
    "\n",
    "data = []\n",
    "labels = ['with_mask', 'without_mask']\n",
    "target = []\n",
    "\n",
    "for label in labels:\n",
    "    folder = os.path.join(output_dir, label)\n",
    "    for file in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (100, 100))\n",
    "            data.append(img)\n",
    "            target.append(labels.index(label))\n",
    "\n",
    "X = np.array(data) / 255.0\n",
    "y = to_categorical(target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "m=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
    "\n",
    "\n",
    "model.save(\"face_mask_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d71898-93f7-44ce-b695-cbbfb7b2c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67a1f152-8758-44b5-ab5f-72418192e737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d598f5-f3b0-4b55-86b9-a6a08e4172e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27115775-e942-4d9e-89e7-bcdc8237b65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (TensorFlow)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
